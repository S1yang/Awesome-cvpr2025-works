# Awesome-cvpr2025-works

---

- [Awesome-cvpr2025-works](#awesome-cvpr2025-works)
  - [未分类](#未分类)
    - [Teaching Large Language Models to Regress Accurate Image Quality Scores Using Score Distribution 训练大型语言模型利用分数分布回归准确的图像质量分数](#teaching-large-language-models-to-regress-accurate-image-quality-scores-using-score-distribution-训练大型语言模型利用分数分布回归准确的图像质量分数)
      - [link:https://arxiv.org/html/2501.11561v2](#linkhttpsarxivorghtml250111561v2)
    - [Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking 用于无人机跟踪的相似性引导层自适应视觉变换器](#similarity-guided-layer-adaptive-vision-transformer-for-uav-tracking-用于无人机跟踪的相似性引导层自适应视觉变换器)
      - [link:https://arxiv.org/html/2503.06625v1](#linkhttpsarxivorghtml250306625v1)
    - [ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration ReconDreamer：为驾驶场景制作世界模型 通过在线恢复进行重建](#recondreamer-crafting-world-models-for-driving-scene-reconstruction-via-online-restoration-recondreamer为驾驶场景制作世界模型-通过在线恢复进行重建)
      - [link:https://arxiv.org/html/2411.19548v1](#linkhttpsarxivorghtml241119548v1)
    - [LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant LamRA：大型多模态模型，您的高级检索助手](#lamra-large-multimodal-model-as-your-advanced-retrieval-assistant-lamra大型多模态模型您的高级检索助手)
      - [link:https://code-kunkun.github.io/LamRA/](#linkhttpscode-kunkungithubiolamra)
    - [LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living LLAVIDAL：日常生活活动的大型语言视觉模型](#llavidal-a-large-language-vision-model-for-daily-activities-of-living-llavidal日常生活活动的大型语言视觉模型)
      - [https://arxiv.org/html/2406.09390v3](#httpsarxivorghtml240609390v3)
    - [EventGPT: Event Stream Understanding with Multimodal Large Language Models EventGPT：通过多模式理解事件流 大型语言模型](#eventgpt-event-stream-understanding-with-multimodal-large-language-models-eventgpt通过多模式理解事件流-大型语言模型)
      - [https://arxiv.org/html/2412.00832v1](#httpsarxivorghtml241200832v1)
    - [DeepCompress-ViT: Rethinking Model Compression to Enhance Efficiency of Vision Transformers at the Edge DeepCompress-ViT：重新思考模型压缩以提高边缘视觉转换器的效率](#deepcompress-vit-rethinking-model-compression-to-enhance-efficiency-of-vision-transformers-at-the-edge-deepcompress-vit重新思考模型压缩以提高边缘视觉转换器的效率)
      - [link:未开源](#link未开源)
    - [Apollo: An Exploration of Video Understanding in Large Multimodal Models 阿波罗：大型多模态模型中视频理解的探索](#apollo-an-exploration-of-video-understanding-in-large-multimodal-models-阿波罗大型多模态模型中视频理解的探索)
      - [link:https://arxiv.org/html/2412.10360v1](#linkhttpsarxivorghtml241210360v1)
    - [Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models 无声品牌攻击：针对文本到图像扩散模型的无触发数据中毒攻击](#silent-branding-attack-trigger-free-data-poisoning-attack-on-text-to-image-diffusion-models-无声品牌攻击针对文本到图像扩散模型的无触发数据中毒攻击)
      - [link:https://silent-branding.github.io/ (代码还未开源)](#linkhttpssilent-brandinggithubio-代码还未开源)
    - [Question-Aware Gaussian Experts for Audio-Visual Question Answering 用于视听问答的问题感知高斯专家](#question-aware-gaussian-experts-for-audio-visual-question-answering-用于视听问答的问题感知高斯专家)
      - [link:https://aim-skku.github.io/QA-TIGER/](#linkhttpsaim-skkugithubioqa-tiger)
    - [IDEA-Bench: How Far are Generative Models from Professional Designing? IDEA-Bench：生成模型距离专业设计还有多远？](#idea-bench-how-far-are-generative-models-from-professional-designing-idea-bench生成模型距离专业设计还有多远)
      - [link:https://github.com/ali-vilab/IDEA-Bench](#linkhttpsgithubcomali-vilabidea-bench)
    - [Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation Crab：统一的视听场景理解模型 明确合作](#crab-a-unified-audio-visual-scene-understanding-model-with-explicit-cooperation-crab统一的视听场景理解模型-明确合作)
      - [link:https://arxiv.org/html/2503.13068v1 https://github.com/GeWu-Lab/Crab](#linkhttpsarxivorghtml250313068v1-httpsgithubcomgewu-labcrab)
    - [Spiking Transformer: Introducing Accurate Addition-Only Spiking Self-Attention for Transformer 尖峰变压器： 为 Transformer 引入精确加法脉冲自注意力机制](#spiking-transformer-introducing-accurate-addition-only-spiking-self-attention-for-transformer-尖峰变压器-为-transformer-引入精确加法脉冲自注意力机制)
      - [link:https://arxiv.org/html/2503.00226v2 (未开源代码实现)](#linkhttpsarxivorghtml250300226v2-未开源代码实现)
    - [Multi-modal Vision Pre-training for Medical Image Analysis 用于医学图像分析的多模态视觉预训练](#multi-modal-vision-pre-training-for-medical-image-analysis-用于医学图像分析的多模态视觉预训练)
      - [link:https://github.com/shaohao011/BrainMVP cvpr-highlight](#linkhttpsgithubcomshaohao011brainmvp-cvpr-highlight)
    - [Rethinking Temporal Fusion with A Unified Gradient Descent View for 3D Semantic Occupancy Prediction 用统一梯度下降视图重新思考时间融合在 3D 语义占用预测中的应用](#rethinking-temporal-fusion-with-a-unified-gradient-descent-view-for-3d-semantic-occupancy-prediction-用统一梯度下降视图重新思考时间融合在-3d-语义占用预测中的应用)
      - [link:未开源](#link未开源-1)
    - [Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models Insight-V：利用多模态大型语言模型探索长链视觉推理](#insight-v-exploring-long-chain-visual-reasoning-with-multimodal-large-language-models-insight-v利用多模态大型语言模型探索长链视觉推理)
      - [link:https://github.com/dongyh20/Insight-V cvpr-highlight](#linkhttpsgithubcomdongyh20insight-v-cvpr-highlight)
    - [Spacetime Understanding in Multimodal Language Model 粗略对应关系在多模态语言模型中引发三维时空理解](#spacetime-understanding-in-multimodal-language-model-粗略对应关系在多模态语言模型中引发三维时空理解)
      - [link:https://arxiv.org/html/2408.00754v2 代码未开源](#linkhttpsarxivorghtml240800754v2-代码未开源)
    - [OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective Fusion OV-DINO：具有语言感知选择性融合的统一开放词汇检测](#ov-dino-unified-open-vocabulary-detection-with-language-aware-selective-fusion-ov-dino具有语言感知选择性融合的统一开放词汇检测)
      - [link:https://arxiv.org/html/2407.07844v1](#linkhttpsarxivorghtml240707844v1)
    - [EgoLife: Towards Egocentric Life Assistant EgoLife：迈向自我中心的生活助手](#egolife-towards-egocentric-life-assistant-egolife迈向自我中心的生活助手)
      - [link:https://huggingface.co/papers/2503.03803](#linkhttpshuggingfacecopapers250303803)
    - [MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis MMAudio：驯服多模式联合训练 用于高质量视频到音频合成](#mmaudio-taming-multimodal-joint-training-for-high-quality-video-to-audio-synthesis-mmaudio驯服多模式联合训练-用于高质量视频到音频合成)
      - [link:https://github.com/hkchengrex/MMAudio](#linkhttpsgithubcomhkchengrexmmaudio)
    - [Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons 通过检测和探索特定任务的神经元来理解 LLMs 的多任务学习（泛化）](#towards-understanding-multi-task-learning-generalization-of-llms-via-detecting-and-exploring-task-specific-neurons-通过检测和探索特定任务的神经元来理解-llms-的多任务学习泛化)
      - [link:https://arxiv.org/html/2407.06488v2 (仅有论文)](#linkhttpsarxivorghtml240706488v2-仅有论文)
    - [BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices BlueLM-V-3B：移动设备上多模态大型语言模型的算法与系统协同设计](#bluelm-v-3b-algorithm-and-system-co-design-for-multimodal-large-language-models-on-mobile-devices-bluelm-v-3b移动设备上多模态大型语言模型的算法与系统协同设计)
    - [SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization SymDPO：通过符号演示直接偏好优化促进大型多模态模型的上下文学习](#symdpo-boosting-in-context-learning-of-large-multimodal-models-with-symbol-demonstration-direct-preference-optimization-symdpo通过符号演示直接偏好优化促进大型多模态模型的上下文学习)
      - [link:https://github.com/APiaoG/SymDPO](#linkhttpsgithubcomapiaogsymdpo)
  - [Hallucination 相关文章](#hallucination-相关文章)
    - [Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key 通过 DPO 缓解大型视觉语言模型中的幻觉： 政策数据是关键](#mitigating-hallucinations-in-large-vision-language-models-via-dpo-on-policy-data-hold-the-key-通过-dpo-缓解大型视觉语言模型中的幻觉-政策数据是关键)
      - [link:https://github.com/zhyang2226/OPA-DPO cvpr-Oral](#linkhttpsgithubcomzhyang2226opa-dpo-cvpr-oral)
    - [FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray Report Generation Models FactCheXcker：缓解测量幻觉 胸部 X 光报告生成模型](#factchexcker-mitigating-measurement-hallucinations-in-chest-x-ray-report-generation-models-factchexcker缓解测量幻觉-胸部-x-光报告生成模型)
      - [link:https://arxiv.org/html/2411.18672v1#S1.T1 (代码未开源)](#linkhttpsarxivorghtml241118672v1s1t1-代码未开源)
    - [VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding VidHalluc：评估多模态大型语言模型中的时间幻觉以理解视频](#vidhalluc-evaluating-temporal-hallucinations-in-multimodal-large-language-models-for-video-understanding-vidhalluc评估多模态大型语言模型中的时间幻觉以理解视频)
      - [link:https://people-robots.github.io/vidhalluc/](#linkhttpspeople-robotsgithubiovidhalluc)
    - [Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens 大型视觉语言模型中间层的“魔鬼”：通过注意力透镜解释、检测和缓解物体幻觉](#devils-in-middle-layers-of-large-vision-language-models-interpreting-detecting-and-mitigating-object-hallucinations-via-attention-lens-大型视觉语言模型中间层的魔鬼通过注意力透镜解释检测和缓解物体幻觉)
      - [link:https://github.com/ZhangqiJiang07/middle_layers_indicating_hallucinations](#linkhttpsgithubcomzhangqijiang07middle_layers_indicating_hallucinations)
    - [ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models ODE：多模态大型语言模型中幻觉的开放集评估](#ode-open-set-evaluation-of-hallucinations-in-multimodal-large-language-models-ode多模态大型语言模型中幻觉的开放集评估)
      - [link:https://arxiv.org/html/2409.09318v3 （代码未开源）](#linkhttpsarxivorghtml240909318v3-代码未开源)
    - [Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention 结合全局和局部注意力机制缓解大型视觉语言模型中的物体幻觉](#mitigating-object-hallucinations-in-large-vision-language-models-with-assembly-of-global-and-local-attention-结合全局和局部注意力机制缓解大型视觉语言模型中的物体幻觉)
      - [link:https://github.com/Lackel/AGLA](#linkhttpsgithubcomlackelagla)
    - [Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection Nullu：通过 HalluSpace 投影缓解大型视觉语言模型中的物体幻觉](#nullu-mitigating-object-hallucinations-in-large-vision-language-models-via-halluspace-projection-nullu通过-halluspace-投影缓解大型视觉语言模型中的物体幻觉)
      - [link:https://github.com/Ziwei-Zheng/Nullu](#linkhttpsgithubcomziwei-zhengnullu)
    - [HalLoc: Token-level Localization of Hallucinations for Vision Language Models HalLoc：视觉语言模型的幻觉标记级定位](#halloc-token-level-localization-of-hallucinations-for-vision-language-models-halloc视觉语言模型的幻觉标记级定位)
      - [link:代码/论文未开源](#link代码论文未开源)
    - [Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception 解毒剂：缓解反事实预设和对象感知中 LVLM 幻觉的统一框架](#antidote-a-unified-framework-for-mitigating-lvlm-hallucinations-in-counterfactual-presupposition-and-object-perception-解毒剂缓解反事实预设和对象感知中-lvlm-幻觉的统一框架)
      - [link:代码/论文未开源](#link代码论文未开源-1)
    - [Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding 看得更远、更清楚：利用注意力因果解码缓解 MLLM 中的幻觉](#seeing-far-and-clearly-mitigating-hallucinations-in-mllms-with-attention-causal-decoding-看得更远更清楚利用注意力因果解码缓解-mllm-中的幻觉)
      - [link:代码/论文未开源](#link代码论文未开源-2)

---

## 未分类

### Teaching Large Language Models to Regress Accurate Image Quality Scores Using Score Distribution 训练大型语言模型利用分数分布回归准确的图像质量分数

#### link:https://arxiv.org/html/2501.11561v2

创新点：用多模态大语言模型进行图像质量的评估

### Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking 用于无人机跟踪的相似性引导层自适应视觉变换器

#### link:https://arxiv.org/html/2503.06625v1

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image.png)

创新点：VIT 进行无人机目标跟踪 轻量化可部署在低算力设备上 动态地禁用了大量表征相似的层，只保留一个最优层 达到 SOTA

### ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration ReconDreamer：为驾驶场景制作世界模型 通过在线恢复进行重建

#### link:https://arxiv.org/html/2411.19548v1

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-1.png)

创新点：使用世界模型知识(LLM-enhanced)来增强驾驶场景重建,在线恢复、渐进式数据更新以保证大范围机动中高质量的渲染。

### LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant LamRA：大型多模态模型，您的高级检索助手

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-3.png)
![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-4.png)

创新点：生成式大型多模态模型 (LMM) 重新用于检索

#### link:https://code-kunkun.github.io/LamRA/

### LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living LLAVIDAL：日常生活活动的大型语言视觉模型

#### https://arxiv.org/html/2406.09390v3

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-5.png)

创新点：数据集指令调优+多模态渐进式训练策略对齐多模态数据=理解日常生活活动的 LLVM

### EventGPT: Event Stream Understanding with Multimodal Large Language Models EventGPT：通过多模式理解事件流 大型语言模型

#### https://arxiv.org/html/2412.00832v1

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-6.png)

创新点：用于事件相机创建的事件流的多模态大语言模型

### DeepCompress-ViT: Rethinking Model Compression to Enhance Efficiency of Vision Transformers at the Edge DeepCompress-ViT：重新思考模型压缩以提高边缘视觉转换器的效率

#### link:未开源

可用点：压缩 VIT 以适应低开销场景

### Apollo: An Exploration of Video Understanding in Large Multimodal Models 阿波罗：大型多模态模型中视频理解的探索

#### link:https://arxiv.org/html/2412.10360v1

创新点：通过研究在以往小模型上针对视频感知模型的方法，提取出能够用于大模型的方法，设计了一个新的用于长视频感知的大语言模型

### Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models 无声品牌攻击：针对文本到图像扩散模型的无触发数据中毒攻击

#### link:https://silent-branding.github.io/ (代码还未开源)

创新点：开发了一种自动数据中毒算法，该算法可以不引人注目地将徽标注入原始图像（数据集）中，确保它们自然融合且不被发现。在此中毒数据集上训练的模型可以生成包含徽标的图像，而不会降低图像质量或文本对齐。

### Question-Aware Gaussian Experts for Audio-Visual Question Answering 用于视听问答的问题感知高斯专家

#### link:https://aim-skku.github.io/QA-TIGER/

![alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-7.png)

创新点：混合专家+高斯加权机制对连续的时间动态建模，自适应地捕捉与问题相关的视听线索并改善时间对其。完成视听问答任务。

### IDEA-Bench: How Far are Generative Models from Professional Designing? IDEA-Bench：生成模型距离专业设计还有多远？

#### link:https://github.com/ali-vilab/IDEA-Bench

关注点：一个用于评估 MMLM 生成图像任务专业能力的测试基准。

### Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation Crab：统一的视听场景理解模型 明确合作

#### link:https://arxiv.org/html/2503.13068v1 https://github.com/GeWu-Lab/Crab

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-8.png)

创新点：显式的任务合作+构建了一个具有显式推理过程的视听统一指令调优数据集 AV-UIE+设计了一个交互感知的 LoRA 结构

### Spiking Transformer: Introducing Accurate Addition-Only Spiking Self-Attention for Transformer 尖峰变压器： 为 Transformer 引入精确加法脉冲自注意力机制

#### link:https://arxiv.org/html/2503.00226v2 (未开源代码实现)

关注点：脉冲神经网络+transformer，旨在降低能耗。

### Multi-modal Vision Pre-training for Medical Image Analysis 用于医学图像分析的多模态视觉预训练

#### link:https://github.com/shaohao011/BrainMVP cvpr-highlight

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-9.png)

关注点：多个跨模态表征学习代理任务+模态数据蒸馏任务 以提高模型多模态的能力

### Rethinking Temporal Fusion with A Unified Gradient Descent View for 3D Semantic Occupancy Prediction 用统一梯度下降视图重新思考时间融合在 3D 语义占用预测中的应用

#### link:未开源

关注点：occ、语义时间融合

### Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models Insight-V：利用多模态大型语言模型探索长链视觉推理

#### link:https://github.com/dongyh20/Insight-V cvpr-highlight

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-10.png)

关注点：利用 CoT 提示增强 MLLM 完成视觉为中心的感知推理任务

### Spacetime Understanding in Multimodal Language Model 粗略对应关系在多模态语言模型中引发三维时空理解

#### link:https://arxiv.org/html/2408.00754v2 代码未开源

关注点：又一篇增强 MLLM 完成 3D 时空感知的任务

### OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective Fusion OV-DINO：具有语言感知选择性融合的统一开放词汇检测

#### link:https://arxiv.org/html/2407.07844v1

DINO 系列的新作，该领域没有太大的挖掘空间

### EgoLife: Towards Egocentric Life Assistant EgoLife：迈向自我中心的生活助手

#### link:https://huggingface.co/papers/2503.03803

偏工程化实践的文章，使用智能眼镜+摄像头+毫米波雷达，设计一个基于自我中心数据的视觉 LLM+检索增强生成模块。整篇论文偏工作量大并有一定的应用场景。可以考虑在该应用场景下进行更深的探索。

### MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis MMAudio：驯服多模式联合训练 用于高质量视频到音频合成

#### link:https://github.com/hkchengrex/MMAudio

github 1.4k stars ,训练从视频中生成声音，很新颖且有价值的工作。

### Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons 通过检测和探索特定任务的神经元来理解 LLMs 的多任务学习（泛化）

#### link:https://arxiv.org/html/2407.06488v2 (仅有论文)

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-11.png)
关注点：论文描述了 LLMS 中特定神经元对任务的贡献，通过关注对当前任务的特定神经元的微调，解决灾难和泛化性遗忘，并给出了 LLMS 可解释性的新观点。从识别特定神经元处出发，或许能有效地解决幻觉问题。

### BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices BlueLM-V-3B：移动设备上多模态大型语言模型的算法与系统协同设计

部署在移动设备上的 Mllm，轻量化的 llm 设计

### SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization SymDPO：通过符号演示直接偏好优化促进大型多模态模型的上下文学习

#### link:https://github.com/APiaoG/SymDPO

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-12.png)

许多大模型在进行视觉语义任务时会偏好于直接忽略视觉信息，仅从文本提示生成响应。
通过在 ICD 图像描述蒸馏 中将文本答案替换成不相关的随机字符，迫使模型学习视觉-符号的映射而不是直接偏心地学习语义信息，从而强化视觉理解。

## Hallucination 相关文章

### Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key 通过 DPO 缓解大型视觉语言模型中的幻觉： 政策数据是关键

#### link:https://github.com/zhyang2226/OPA-DPO cvpr-Oral

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-13.png)
方法简述：1.生成初始响应：​ 使用原始模型根据图像和提示生成响应。2.专家修订：​ 利用 GPT-4V 对生成的响应进行评估和修订，纠正幻觉内容。3.策略内对齐（OPA）：​ 通过低秩适配（LoRA）微调，将原始响应和修订后的响应对齐，使其成为策略内数据。4.DPO 训练：​ 使用对齐后的数据进行 DPO 训练，进一步优化模型。

### FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray Report Generation Models FactCheXcker：缓解测量幻觉 胸部 X 光报告生成模型

#### link:https://arxiv.org/html/2411.18672v1#S1.T1 (代码未开源)

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-14.png)

FactCheXcker 由三个主要组件组成：查询生成器、代码生成器和报告更新器。当收到一张医学图像及其对应的 AI 生成的可能包含幻觉测量值的报告时，查询生成器会识别报告中潜在的测量差异，代码生成器会创建并执行专门的代码以从图像中获取精确的测量值，报告更新器会将这些经过验证的测量值集成到报告中。

### VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding VidHalluc：评估多模态大型语言模型中的时间幻觉以理解视频

#### link:https://people-robots.github.io/vidhalluc/

用于检验 MLLM 视频理解中幻觉的最大基准测试。它包含 5,002 个视频，并进行配对以突出显示容易出现幻觉的情况。
此外，提出了 DINO-HEAL，这是一种无需训练的方法，它通过结合 DINOv2 的空间显著性信息来在推理过程中重新加权视觉特征，从而减少幻觉。我们的结果表明，DINO-HEAL 持续提升了 VidHalluc 的性能，在所有任务中，减轻幻觉的效果平均提高了 3.02%。

### Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens 大型视觉语言模型中间层的“魔鬼”：通过注意力透镜解释、检测和缓解物体幻觉

#### link:https://github.com/ZhangqiJiang07/middle_layers_indicating_hallucinations

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-17.png)

大型视觉语言模型 (LVLM) 中的幻觉显著降低了其可靠性，促使研究人员探索幻觉的成因。然而，大多数研究主要关注语言层面，而非视觉层面。本文探讨 LVLM 如何处理视觉信息，以及该过程是否会导致幻觉。
首先，我们使用注意力透镜识别 LVLM 处理视觉数据的阶段，发现中间层至关重要。此外，我们发现这些层可以进一步细分为两个阶段：“视觉信息丰富”和“语义细化”，分别将视觉数据传播为对象标记并通过文本进行解读。通过分析视觉信息丰富阶段的注意力模式，我们发现真实标记始终比幻觉标记获得更高的注意力权重，这可以作为幻觉的有力指标。进一步研究多头注意力图谱发现，幻觉标记通常是由头部与不一致的物体交互产生的。
基于这些见解，我们提出了一种简单的推理时间方法，通过整合各个头脑的信息来调整视觉注意力。 大量实验表明，这种方法可以有效缓解主流 LVLM 中的幻觉，且无需额外的训练成本。

### ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models ODE：多模态大型语言模型中幻觉的开放集评估

#### link:https://arxiv.org/html/2409.09318v3 （代码未开源）

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-15.png)

幻觉对多模态大型语言模型 (MLLM) 构成了持续挑战。然而，现有的幻觉评估基准通常是静态的，这可能会忽略数据污染的潜在风险。为了解决这个问题，我们提出了 ODE，这是一个开放集的动态协议，旨在在存在性和属性层面评估 MLLM 中的对象幻觉。ODE 采用基于图的结构来表示现实世界中的对象概念、它们的属性以及它们之间的分布关联。该结构有助于基于不同的分布标准提取概念组合，从而为结构化查询生成多样化的样本，用于评估生成和判别任务中的幻觉。通过生成新样本、动态概念组合和不同的分布频率，ODE 降低了数据污染的风险并拓宽了评估范围。该协议适用于一般场景和特殊场景，包括数据有限的场景。实验结果证明了我们协议的有效性，表明使用 ODE 生成的样本进行评估时，MLLM 表现出更高的幻觉率，这表明存在潜在的数据污染。此外，这些生成的样本有助于分析幻觉模式和微调模型，为减轻 MLLM 中的幻觉提供有效的方法。

通过构建动态开放集测试评估 MLLM 幻觉

### Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention 结合全局和局部注意力机制缓解大型视觉语言模型中的物体幻觉

#### link:https://github.com/Lackel/AGLA

提出了全局和局部注意力组合 (AGLA) ，这是一种无需训练、即插即用的方法，旨在通过结合用于响应生成的全局特征和用于视觉判别的局部特征来减少物体幻觉。我们大量的实验表明，AGLA 能够持续缓解物体幻觉，并提升 LVLM 在各种判别性和生成性基准测试中的整体感知能力。

### Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection Nullu：通过 HalluSpace 投影缓解大型视觉语言模型中的物体幻觉

#### link:https://github.com/Ziwei-Zheng/Nullu

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-16.png)

大型视觉语言模型 (LVLM) 经常受到对象幻觉 (OH) 的影响。为了缓解此问题，我们提出了一种基于不安全子空间（本文中我们称之为 HalluSpace）编辑模型权重的有效方法。通过真实文本提示和幻觉文本提示与视觉内容一起作为输入，我们可以通过提取幻觉嵌入特征并移除 LVLM 中的真实表示来识别 HalluSpace。通过正交化模型权重，输入特征将被投影到 HalluSpace 的零空间 (Null space) 中以减少 OH，基于此我们将该方法命名为 Nullu。
我们发现，在用于构建 LVLM 的大型语言模型 (LLMs) 中，HalluSpace 通常包含先验信息，而先前的研究表明，这些信息是导致 OH 的关键原因。因此，零空间投影可以抑制 LLMs 的先验信息，从而滤除幻觉特征，最终获得上下文准确的输出。

### HalLoc: Token-level Localization of Hallucinations for Vision Language Models HalLoc：视觉语言模型的幻觉标记级定位

#### link:代码/论文未开源

本研究引入了 HalLoc-Bench，这是首个专为幻觉定位设计的基准测试集。HalLoc-Bench 支持针对各种幻觉类型（对象、属性、关系、场景）和任务（VQA、字幕、指令遵循）进行定位训练和评估。我们还提出了 HalLoc，一个简单而有效的定位器，它为 HalLoc-Bench 奠定了坚实的基础。实验表明，HalLoc-Bench 能够有效地评估幻觉定位，为该领域的发展提供了宝贵的工具。我们的分析进一步表明，更精准的幻觉定位可以提升大型视觉语言模型中幻觉的评估和缓解效果。

### Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception 解毒剂：缓解反事实预设和对象感知中 LVLM 幻觉的统一框架

#### link:代码/论文未开源

本文探讨了 LVLM 在解决反事实预设问题 (CPQ) 时的脆弱性，在这些问题中，模型容易接受反事实对象的预设并产生严重的幻觉响应。为此，我们引入了“Antidote”，这是一个统一的、由合成数据驱动的训练后框架，用于缓解上述两种类型的幻觉。它利用合成数据将事实先验融入问题中，实现自我修正，并将缓解过程解耦为偏好优化问题。此外，我们构建了“CP-Bench”，这是一个新的基准，用于评估 LVLM 正确处理 CPQ 和生成事实响应的能力。应用于 LLaVA 系列，Antidote 可以同时将 CP-Bench 的性能提高 50% 以上，将 POPE 的性能提高 1.8-3.3%，将 CHAIR 和 SHR 的性能提高 30-50%，所有这些都无需依赖更强大的 LVLM 或人类反馈的外部监督，也不会引入明显的灾难性遗忘问题。

### Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding 看得更远、更清楚：利用注意力因果解码缓解 MLLM 中的幻觉

#### link:代码/论文未开源

多模态大型语言模型 (MLLM) 的最新进展显著提升了视觉问答系统的性能。本文将幻觉分为两大类：初始幻觉和滚雪球幻觉。我们认为，可以从标记交互过程中直接提取充足的语境信息。受解码策略中因果推理的启发，我们提出利用因果掩码来建立多模态标记之间的信息传播。假设这些标记之间交互不足可能导致模型依赖异常标记，从而忽略密集且丰富的语境线索。因此，我们提出通过处理异常标记来干预传播过程，以增强语境推理。
为此，我们提出了 FarSight，这是一种多功能的即插即用解码策略，仅通过优化因果掩码即可减少异常标记对注意力的干扰。我们方法的核心是有效的标记传播。我们在因果掩码的上三角 ​​ 矩阵中设计了一个注意力寄存器结构，用于动态分配转移到异常标记的注意力捕获。此外，我们还提出了一种具有递减掩码率的位置感知编码方法，使模型能够关注更靠前的标记，尤其是在视频序列任务中。
