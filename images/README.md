# Awesome-cvpr2025-works

## 未分类

### Teaching Large Language Models to Regress Accurate Image Quality Scores Using Score Distribution 训练大型语言模型利用分数分布回归准确的图像质量分数

#### link:https://arxiv.org/html/2501.11561v2

创新点：用多模态大语言模型进行图像质量的评估

### Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking 用于无人机跟踪的相似性引导层自适应视觉变换器

#### link:https://arxiv.org/html/2503.06625v1

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image.png)

创新点：VIT 进行无人机目标跟踪 轻量化可部署在低算力设备上 动态地禁用了大量表征相似的层，只保留一个最优层 达到 SOTA

### ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration ReconDreamer：为驾驶场景制作世界模型 通过在线恢复进行重建

#### link:https://arxiv.org/html/2411.19548v1

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-1.png)

创新点：使用世界模型知识(LLM-enhanced)来增强驾驶场景重建,在线恢复、渐进式数据更新以保证大范围机动中高质量的渲染。

### LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant LamRA：大型多模态模型，您的高级检索助手

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-3.png)
![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-4.png)

创新点：生成式大型多模态模型 (LMM) 重新用于检索

#### link:https://code-kunkun.github.io/LamRA/

### LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living LLAVIDAL：日常生活活动的大型语言视觉模型

#### https://arxiv.org/html/2406.09390v3

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-5.png)

创新点：数据集指令调优+多模态渐进式训练策略对齐多模态数据=理解日常生活活动的 LLVM

### EventGPT: Event Stream Understanding with Multimodal Large Language Models EventGPT：通过多模式理解事件流 大型语言模型

#### https://arxiv.org/html/2412.00832v1

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-6.png)

创新点：用于事件相机创建的事件流的多模态大语言模型

### DeepCompress-ViT: Rethinking Model Compression to Enhance Efficiency of Vision Transformers at the Edge DeepCompress-ViT：重新思考模型压缩以提高边缘视觉转换器的效率

#### link:未开源

可用点：压缩 VIT 以适应低开销场景

### Apollo: An Exploration of Video Understanding in Large Multimodal Models 阿波罗：大型多模态模型中视频理解的探索

#### link:https://arxiv.org/html/2412.10360v1

创新点：通过研究在以往小模型上针对视频感知模型的方法，提取出能够用于大模型的方法，设计了一个新的用于长视频感知的大语言模型

### Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models 无声品牌攻击：针对文本到图像扩散模型的无触发数据中毒攻击

#### link:https://silent-branding.github.io/ (代码还未开源)

创新点：开发了一种自动数据中毒算法，该算法可以不引人注目地将徽标注入原始图像（数据集）中，确保它们自然融合且不被发现。在此中毒数据集上训练的模型可以生成包含徽标的图像，而不会降低图像质量或文本对齐。

### Question-Aware Gaussian Experts for Audio-Visual Question Answering 用于视听问答的问题感知高斯专家

#### link:https://aim-skku.github.io/QA-TIGER/

![alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-7.png)

创新点：混合专家+高斯加权机制对连续的时间动态建模，自适应地捕捉与问题相关的视听线索并改善时间对其。完成视听问答任务。

### IDEA-Bench: How Far are Generative Models from Professional Designing? IDEA-Bench：生成模型距离专业设计还有多远？

#### link:https://github.com/ali-vilab/IDEA-Bench

关注点：一个用于评估 MMLM 生成图像任务专业能力的测试基准。

### Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation Crab：统一的视听场景理解模型 明确合作

#### link:https://arxiv.org/html/2503.13068v1 https://github.com/GeWu-Lab/Crab

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-8.png)

创新点：显式的任务合作+构建了一个具有显式推理过程的视听统一指令调优数据集 AV-UIE+设计了一个交互感知的 LoRA 结构

### Spiking Transformer: Introducing Accurate Addition-Only Spiking Self-Attention for Transformer 尖峰变压器： 为 Transformer 引入精确加法脉冲自注意力机制

#### link:https://arxiv.org/html/2503.00226v2 (未开源代码实现)

关注点：脉冲神经网络+transformer，旨在降低能耗。

### Multi-modal Vision Pre-training for Medical Image Analysis 用于医学图像分析的多模态视觉预训练

#### link:https://github.com/shaohao011/BrainMVP cvpr-highlight

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-9.png)

关注点：多个跨模态表征学习代理任务+模态数据蒸馏任务 以提高模型多模态的能力

### Rethinking Temporal Fusion with A Unified Gradient Descent View for 3D Semantic Occupancy Prediction 用统一梯度下降视图重新思考时间融合在 3D 语义占用预测中的应用

#### link:未开源

关注点：occ、语义时间融合

### Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models Insight-V：利用多模态大型语言模型探索长链视觉推理

#### link:https://github.com/dongyh20/Insight-V cvpr-highlight

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-10.png)

关注点：利用 CoT 提示增强 MLLM 完成视觉为中心的感知推理任务

### Spacetime Understanding in Multimodal Language Model 粗略对应关系在多模态语言模型中引发三维时空理解

#### link:https://arxiv.org/html/2408.00754v2 代码未开源

关注点：又一篇增强 MLLM 完成 3D 时空感知的任务

### OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective Fusion OV-DINO：具有语言感知选择性融合的统一开放词汇检测

#### link:https://arxiv.org/html/2407.07844v1

DINO 系列的新作，该领域没有太大的挖掘空间

### EgoLife: Towards Egocentric Life Assistant EgoLife：迈向自我中心的生活助手

#### link:https://huggingface.co/papers/2503.03803

偏工程化实践的文章，使用智能眼镜+摄像头+毫米波雷达，设计一个基于自我中心数据的视觉 LLM+检索增强生成模块。整篇论文偏工作量大并有一定的应用场景。可以考虑在该应用场景下进行更深的探索。

### MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis MMAudio：驯服多模式联合训练 用于高质量视频到音频合成

#### link:https://github.com/hkchengrex/MMAudio

github 1.4k stars ,训练从视频中生成声音，很新颖且有价值的工作。

### FilmComposer: LLM-Driven Music Production for Silent Film Clips 无声电影片段的音乐制作

#### link:https://apple-jun.github.io/FilmComposer.github.io/ (code 未发布)

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-23.png)

FilmComposer ，它可以模拟专业音乐家的实际工作流程。FilmComposer 首次将大型生成模型与多智能体方法相结合，充分利用了波形音乐和符号音乐生成的优势。此外，FilmComposer 首次专注于电影音乐制作的三大核心要素——音质、音乐性和音乐发展——并引入了节奏、语义和视觉等各种控制措施来增强这些关键方面。
具体来说，FilmComposer 由视觉处理模块、可控制节奏的 MusicGen 以及多智能体评估、编曲和混音模块组成。此外，我们的框架可以无缝集成到实际的音乐制作流程中，并允许用户干预每个步骤，从而提供强大的交互性和高度的创作自由。
此外，鉴于目前专业高质量的电影音乐数据集的匮乏，我们提出了 MusicPro-7k ，其中包含 7,418 个电影片段、音乐、描述、节奏点和主旋律。

### Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons 通过检测和探索特定任务的神经元来理解 LLMs 的多任务学习（泛化）

#### link:https://arxiv.org/html/2407.06488v2 (仅有论文)

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-11.png)
关注点：论文描述了 LLMS 中特定神经元对任务的贡献，通过关注对当前任务的特定神经元的微调，解决灾难和泛化性遗忘，并给出了 LLMS 可解释性的新观点。从识别特定神经元处出发，或许能有效地解决幻觉问题。

### BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices BlueLM-V-3B：移动设备上多模态大型语言模型的算法与系统协同设计

部署在移动设备上的 Mllm，轻量化的 llm 设计

### SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization SymDPO：通过符号演示直接偏好优化促进大型多模态模型的上下文学习

#### link:https://github.com/APiaoG/SymDPO

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-12.png)

许多大模型在进行视觉语义任务时会偏好于直接忽略视觉信息，仅从文本提示生成响应。
通过在 ICD 图像描述蒸馏 中将文本答案替换成不相关的随机字符，迫使模型学习视觉-符号的映射而不是直接偏心地学习语义信息，从而强化视觉理解。

### Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering 使用自反射标记增强多模态 基于知识的视觉问答

#### link:https://github.com/aimagelab/ReflectiVA

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-19.png)
![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-18.png)
利用 Self-Reflective Tokens 来确定对外部知识的需求，并检测和预测外部知识的相关性，管理外部知识。来提高 MLLM 的视觉问答性能。

###  VidComposition: Can MLLMs Analyze Compositions in Compiled Videos? VidComposition ：MLLM 可以分析编译视频中的构图吗？

#### link:https://yunlong10.github.io/VidComposition/

推出了 VidComposition，这是一个全新的基准，专门用于评估 MLLM 的视频构图理解能力，它使用精心挑选的整合视频和电影级注释。VidComposition 包含 982 个视频和 1706 个多项选择题，涵盖了各种构图方面，例如摄像机运动、角度、镜头大小、叙事结构、人物动作和情绪等。

### ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems ComfyBench：ComfyUI 中基于基准测试的代理，用于自主设计协作式 AI 系统

#### link:https://xxyqwq.github.io/ComfyBench/

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-20.png)

(a) ComfyBench 是一个全面的基准测试，用于评估智能体在 ComfyUI 中设计协作 AI 系统的能力。给定任务指令，智能体需要从文档中学习并创建工作流来描述协作 AI 系统。性能通过通过率和解决率来衡量，反映工作流是否能够正确执行以及任务要求是否符合要求。 实现。(b) ComfyAgent 通过生成工作流在 ComfyUI 中构建协作式 AI 系统。工作流转换为等效代码，以便更好地理解它们。ComfyAgent 可以从中学习 现有的工作流程，并自主设计新的工作流程。生成的工作流程可以解释为 协作人工智能系统来完成给定的任务。

### \model: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos 基于树的自适应视频表示长视频推理

#### link:https://videotree2024.github.io/

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-21.png)

具体来说，VideoTree 会从输入视频中动态提取与查询相关的信息，并构建基于树状结构的视频表示以进行推理。首先，VideoTree 会根据帧的视觉特征对其进行聚类，并根据其与查询的相关性对聚类进行评分，从而自适应地选择用于字幕的帧。我们不断重复此过程，直到提取出足够多的与查询相关的关键帧。其次，它将视觉聚类组织成查询自适应、分层的树形结构；树的结构编码了不同级别的粒度，相关片段的分辨率更高（更深）。 最后，VideoTree 通过遍历树的关键帧并将其字幕传递给负责回答查询的应答模型，为每个问题生成答案。

### Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference 揭开 MLLM 中视觉信息流的面纱：解锁更快推理的途径

#### link:https://github.com/ustc-hyin/HiMAP

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-22.png)

本文揭示了视觉信息主导流向的转变：(1) 在浅层，图像标记和指令标记之间存在强交互，其中大多数视觉信息被注入指令标记中，以形成跨模态语义表征；(2) 在较深的层中，图像标记主要相互作用，其余视觉信息聚合起来以优化视觉模态内的语义表征。基于这些洞察，我们提出了分层模态感知剪枝 (HiMAP)，这是一种即插即用的推理加速方法，它可以在特定层动态剪枝图像标记，在不牺牲性能的情况下将计算成本降低约 65%。

### Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly 揭秘 MLLM 的无知：看得清楚，回答得错误

#### link:https://github.com/BAAI-DCAI/MMVU

多模态大型语言模型 (MLLM) 在多模态任务中表现出色，尤其是在视觉理解方面。然而，我们发现，即使 MLLM 理解了视觉内容，它们也经常会生成错误的答案。
我们的研究发现了两个主要问题：1）大多数指令调优数据集主要包含与视觉内容“直接”相关的问题，导致 MLLM 对其他间接问题的回答存在偏差；2）MLLM 对视觉标记的注意力明显低于对系统标记和问题标记的注意力。
我们进一步观察到，问题与视觉标记之间的注意力得分以及模型对答案的置信度在回答误导性问题时低于回答直接问题时。
为了应对第一个挑战，我们引入了成对的正负数据构建流程，以丰富数据集。对于第二个挑战，我们建议通过细化文本和视觉提示来增强模型在解码过程中对视觉内容的关注。对于文本提示，我们提出了一种内容引导的细化策略，该策略在回答问题之前进行初步的视觉内容分析，以生成结构化信息。 此外，我们采用了一种视觉注意力细化策略，突出显示与问题相关的视觉标记，以增强模型对与问题相关的视觉内容的注意力。

### Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding Video-3D LLM：学习位置感知视频表征，实现 3D 场景理解

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-24.png)

#### link:https://github.com/LaVi-Lab/Video-3D-LLM

Video-3D LLM 能够更准确地将视频表征与现实世界的空间上下文对齐。

## Hallucination 相关文章

### Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key 通过 DPO 缓解大型视觉语言模型中的幻觉： 政策数据是关键

#### link:https://github.com/zhyang2226/OPA-DPO cvpr-Oral

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-13.png)
方法简述：1.生成初始响应：​ 使用原始模型根据图像和提示生成响应。2.专家修订：​ 利用 GPT-4V 对生成的响应进行评估和修订，纠正幻觉内容。3.策略内对齐（OPA）：​ 通过低秩适配（LoRA）微调，将原始响应和修订后的响应对齐，使其成为策略内数据。4.DPO 训练：​ 使用对齐后的数据进行 DPO 训练，进一步优化模型。

### FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray Report Generation Models FactCheXcker：缓解测量幻觉 胸部 X 光报告生成模型

#### link:https://arxiv.org/html/2411.18672v1#S1.T1 (代码未开源)

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-14.png)

FactCheXcker 由三个主要组件组成：查询生成器、代码生成器和报告更新器。当收到一张医学图像及其对应的 AI 生成的可能包含幻觉测量值的报告时，查询生成器会识别报告中潜在的测量差异，代码生成器会创建并执行专门的代码以从图像中获取精确的测量值，报告更新器会将这些经过验证的测量值集成到报告中。

### VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding VidHalluc：评估多模态大型语言模型中的时间幻觉以理解视频

#### link:https://people-robots.github.io/vidhalluc/

用于检验 MLLM 视频理解中幻觉的最大基准测试。它包含 5,002 个视频，并进行配对以突出显示容易出现幻觉的情况。
此外，提出了 DINO-HEAL，这是一种无需训练的方法，它通过结合 DINOv2 的空间显著性信息来在推理过程中重新加权视觉特征，从而减少幻觉。我们的结果表明，DINO-HEAL 持续提升了 VidHalluc 的性能，在所有任务中，减轻幻觉的效果平均提高了 3.02%。

### Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens 大型视觉语言模型中间层的“魔鬼”：通过注意力透镜解释、检测和缓解物体幻觉

#### link:https://github.com/ZhangqiJiang07/middle_layers_indicating_hallucinations

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-17.png)

大型视觉语言模型 (LVLM) 中的幻觉显著降低了其可靠性，促使研究人员探索幻觉的成因。然而，大多数研究主要关注语言层面，而非视觉层面。本文探讨 LVLM 如何处理视觉信息，以及该过程是否会导致幻觉。
首先，我们使用注意力透镜识别 LVLM 处理视觉数据的阶段，发现中间层至关重要。此外，我们发现这些层可以进一步细分为两个阶段：“视觉信息丰富”和“语义细化”，分别将视觉数据传播为对象标记并通过文本进行解读。通过分析视觉信息丰富阶段的注意力模式，我们发现真实标记始终比幻觉标记获得更高的注意力权重，这可以作为幻觉的有力指标。进一步研究多头注意力图谱发现，幻觉标记通常是由头部与不一致的物体交互产生的。
基于这些见解，我们提出了一种简单的推理时间方法，通过整合各个头脑的信息来调整视觉注意力。 大量实验表明，这种方法可以有效缓解主流 LVLM 中的幻觉，且无需额外的训练成本。

### ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models ODE：多模态大型语言模型中幻觉的开放集评估

#### link:https://arxiv.org/html/2409.09318v3 （代码未开源）

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-15.png)

幻觉对多模态大型语言模型 (MLLM) 构成了持续挑战。然而，现有的幻觉评估基准通常是静态的，这可能会忽略数据污染的潜在风险。为了解决这个问题，我们提出了 ODE，这是一个开放集的动态协议，旨在在存在性和属性层面评估 MLLM 中的对象幻觉。ODE 采用基于图的结构来表示现实世界中的对象概念、它们的属性以及它们之间的分布关联。该结构有助于基于不同的分布标准提取概念组合，从而为结构化查询生成多样化的样本，用于评估生成和判别任务中的幻觉。通过生成新样本、动态概念组合和不同的分布频率，ODE 降低了数据污染的风险并拓宽了评估范围。该协议适用于一般场景和特殊场景，包括数据有限的场景。实验结果证明了我们协议的有效性，表明使用 ODE 生成的样本进行评估时，MLLM 表现出更高的幻觉率，这表明存在潜在的数据污染。此外，这些生成的样本有助于分析幻觉模式和微调模型，为减轻 MLLM 中的幻觉提供有效的方法。

通过构建动态开放集测试评估 MLLM 幻觉

### Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention 结合全局和局部注意力机制缓解大型视觉语言模型中的物体幻觉

#### link:https://github.com/Lackel/AGLA

提出了全局和局部注意力组合 (AGLA) ，这是一种无需训练、即插即用的方法，旨在通过结合用于响应生成的全局特征和用于视觉判别的局部特征来减少物体幻觉。我们大量的实验表明，AGLA 能够持续缓解物体幻觉，并提升 LVLM 在各种判别性和生成性基准测试中的整体感知能力。

### Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection Nullu：通过 HalluSpace 投影缓解大型视觉语言模型中的物体幻觉

#### link:https://github.com/Ziwei-Zheng/Nullu

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-16.png)

大型视觉语言模型 (LVLM) 经常受到对象幻觉 (OH) 的影响。为了缓解此问题，我们提出了一种基于不安全子空间（本文中我们称之为 HalluSpace）编辑模型权重的有效方法。通过真实文本提示和幻觉文本提示与视觉内容一起作为输入，我们可以通过提取幻觉嵌入特征并移除 LVLM 中的真实表示来识别 HalluSpace。通过正交化模型权重，输入特征将被投影到 HalluSpace 的零空间 (Null space) 中以减少 OH，基于此我们将该方法命名为 Nullu。
我们发现，在用于构建 LVLM 的大型语言模型 (LLMs) 中，HalluSpace 通常包含先验信息，而先前的研究表明，这些信息是导致 OH 的关键原因。因此，零空间投影可以抑制 LLMs 的先验信息，从而滤除幻觉特征，最终获得上下文准确的输出。

### MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations MASH-VLM：缓解视频中的动作场景幻觉 通过解开时空表征

#### link:https://arxiv.org/html/2503.15871v1（代码未开源）

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-25.png)

我们解决了视频大型语言模型 (Video-LLMs) 中的动作场景幻觉问题。该模型会根据场景上下文错误地预测动作，或根据观察到的动作预测场景。我们观察到，现有的 Video-LLMs 经常受到动作场景幻觉的影响，主要有两个原因。首先，现有的 Video-LLMs 通过对所有标记应用注意操作来混合空间和时间特征。其次，它们使用标准旋转位置嵌入 (RoPE)，这会导致文本标记根据其顺序过分强调某些类型的标记。
为了解决这些问题，我们引入了 MASH-VLM ，通过解开时空表征来缓解 Video-LLMs 中的动作场景幻觉。 我们的方法包括两项关键创新： （1）DST-attention，一种新颖的注意力机制，它通过使用掩蔽注意力来限制空间和时间标记之间的直接交互，从而解开 LLM 内的空间和时间标记； （2）Harmonic-RoPE，扩展了位置 ID 的维度，使得空间和时间标记相对于文本标记保持平衡的位置。 为了评估 Video-LLMs 中的动作场景幻觉，我们引入了 UNSCENE 基准，其中包含 1,320 个视频和 4,078 个 QA 对。 MASH-VLM 在 UNSCENE 基准以及现有的视频理解基准上实现了最先进的性能。

### Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices 多模态中的多层视觉特征融合 LLMs： 方法、分析和最佳实践

#### link:https://github.com/EIT-NLP/Layer_Select_Fuse_for_MLLM

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-26.png)

近年来，多模态大型语言模型 (MLLM) 取得了显著进展，其中视觉特征在提升模型性能方面发挥着越来越重要的作用。然而，MLLM 中多层视觉特征的融合仍未得到充分探索，尤其是在最优层选择和融合策略方面。现有方法通常依赖于任意的设计选择，导致结果不尽理想。
本文系统地研究了多层视觉特征融合的两个核心方面：(1) 选择最有效的视觉层；(2) 确定与语言模型的最佳融合方法。我们的实验表明，虽然融合来自多个阶段的视觉特征可以提高泛化能力，但在同一阶段中引入额外的特征通常会导致性能下降。此外，我们发现，在输入阶段直接融合多层视觉特征，在各种配置下都能始终获得更卓越、更稳定的性能。

### DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation DiffSensei：桥接多模态模型和扩散模型 用于定制漫画生成

### link:https://jianzongwu.github.io/projects/diffsensei/

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-27.png)
趣味性比较强的工作

### Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction 通过解耦的感知、决策和反应，实现视频的主动实时交互

#### link:https://github.com/Mark12Ding/Dispider

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-28.png)

主动实时视频交互 LLMs 为人机交互引入了一种新的范式，其中模型不仅可以理解用户意图，还可以在连续处理流视频的同时做出响应。
与在回答问题之前分析整个视频的离线视频 LLMs 不同，主动实时交互需要三种能力：1）感知：实时视频监控和交互捕捉。2）决策：在适当的情况下发起主动交互，3）反应：与用户持续交互。然而，所需能力之间存在固有冲突。决策和反应需要相反的感知尺度和粒度，而自回归解码在反应过程中阻碍了实时感知和决策。
为了在一个和谐的系统中统一冲突的能力，我们提出了 Dispider，一个解开感知、决策和反应的系统。
Dispider 具有一个轻量级的主动流视频处理模块，可跟踪视频流并识别最佳交互时刻。一旦触发交互，异步交互模块会提供详细的响应，同时处理模块会持续监控视频。我们分离式异步设计确保了响应的及时性、上下文准确性和计算效率，使 Dispider 成为长时间视频流主动实时交互的理想选择。

### Enhancing Video-LLM Reasoning via Agent-of-Thoughts Distillation 通过思维代理提炼增强视频推理

#### link:https://zhengrongz.github.io/AoTD/ (代码未开源)

![Alt text](https://github.com/S1yang/Awesome-cvpr2025-works/blob/main/images/image-29.png)
本文旨在解决视频问答 (VideoQA) 问题。这项任务通常需要多步推理以及对时空动态的深刻理解。虽然大型视频语言模型在基准测试中表现良好，但它们往往缺乏可解释性和时空基础。
在本文中，我们提出了一种基于代理的思想蒸馏 ( AoTD ) 方法，该方法通过将自动生成的思想链 (CoT) 纳入指令调整过程来增强模型。具体而言，我们利用基于代理的系统将复杂问题分解为子任务，并使用专门的视觉模型来处理它们，然后将中间结果视为推理链。我们还引入了一种使用大型语言模型 (LLM) 的验证机制，以确保生成的 CoT 的可靠性。大量实验表明，AoTD 提高了多项选择题和开放式基准测试的性能。

### [占位]Notes-guided MLLM Reasoning: Enhancing MLLM with Knowledge and Visual Notes for Visual Question Answering 笔记引导的 MLLM 推理：利用知识和视觉笔记增强 MLLM 以进行视觉问答

#### link:论文/代码未开源

### HalLoc: Token-level Localization of Hallucinations for Vision Language Models HalLoc：视觉语言模型的幻觉标记级定位

#### link:代码/论文未开源

本研究引入了 HalLoc-Bench，这是首个专为幻觉定位设计的基准测试集。HalLoc-Bench 支持针对各种幻觉类型（对象、属性、关系、场景）和任务（VQA、字幕、指令遵循）进行定位训练和评估。我们还提出了 HalLoc，一个简单而有效的定位器，它为 HalLoc-Bench 奠定了坚实的基础。实验表明，HalLoc-Bench 能够有效地评估幻觉定位，为该领域的发展提供了宝贵的工具。我们的分析进一步表明，更精准的幻觉定位可以提升大型视觉语言模型中幻觉的评估和缓解效果。

### Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception 解毒剂：缓解反事实预设和对象感知中 LVLM 幻觉的统一框架

#### link:代码/论文未开源

本文探讨了 LVLM 在解决反事实预设问题 (CPQ) 时的脆弱性，在这些问题中，模型容易接受反事实对象的预设并产生严重的幻觉响应。为此，我们引入了“Antidote”，这是一个统一的、由合成数据驱动的训练后框架，用于缓解上述两种类型的幻觉。它利用合成数据将事实先验融入问题中，实现自我修正，并将缓解过程解耦为偏好优化问题。此外，我们构建了“CP-Bench”，这是一个新的基准，用于评估 LVLM 正确处理 CPQ 和生成事实响应的能力。应用于 LLaVA 系列，Antidote 可以同时将 CP-Bench 的性能提高 50% 以上，将 POPE 的性能提高 1.8-3.3%，将 CHAIR 和 SHR 的性能提高 30-50%，所有这些都无需依赖更强大的 LVLM 或人类反馈的外部监督，也不会引入明显的灾难性遗忘问题。

### Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding 看得更远、更清楚：利用注意力因果解码缓解 MLLM 中的幻觉

#### link:代码/论文未开源

多模态大型语言模型 (MLLM) 的最新进展显著提升了视觉问答系统的性能。本文将幻觉分为两大类：初始幻觉和滚雪球幻觉。我们认为，可以从标记交互过程中直接提取充足的语境信息。受解码策略中因果推理的启发，我们提出利用因果掩码来建立多模态标记之间的信息传播。假设这些标记之间交互不足可能导致模型依赖异常标记，从而忽略密集且丰富的语境线索。因此，我们提出通过处理异常标记来干预传播过程，以增强语境推理。
为此，我们提出了 FarSight，这是一种多功能的即插即用解码策略，仅通过优化因果掩码即可减少异常标记对注意力的干扰。我们方法的核心是有效的标记传播。我们在因果掩码的上三角 ​​ 矩阵中设计了一个注意力寄存器结构，用于动态分配转移到异常标记的注意力捕获。此外，我们还提出了一种具有递减掩码率的位置感知编码方法，使模型能够关注更靠前的标记，尤其是在视频序列任务中。
